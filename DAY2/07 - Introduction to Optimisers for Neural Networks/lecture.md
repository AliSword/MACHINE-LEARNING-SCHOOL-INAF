Optimisers

-gradient descendent: you don't change learning rate.
-adam and RMSProp: if you are going faster they reduce the learning rate 
(they used an hystorical content with respect to gradient descendetn).
They all three use very different paths.
The lat two can escape local minima.
The best optimiser is Adam.
(optimisers are hyperparameters).
